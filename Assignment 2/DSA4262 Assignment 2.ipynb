{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f8cc0a4",
   "metadata": {},
   "source": [
    "# DSA4262 Data Visualization Individual Assignment 2\n",
    "\n",
    "Cheryl Lee (Li Jia Xuan) A0262272J\n",
    "\n",
    "Link to Github repository: https://github.com/cherylololol/DSA4262"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b243ab",
   "metadata": {},
   "source": [
    "## 1. Motivation\n",
    "\n",
    "Mental health signals are often expressed through subjective, nuanced language rather than explicit clinical markers. In digital environments such as social media platforms, individuals frequently describe stress through informal narratives, emotional vocabulary, and subtle shifts in tone. Translating these unstructured expressions into measurable computational features presents a central challenge in Artificial Intelligence (AI) for mental health.\n",
    "\n",
    "The Dreaddit dataset provides annotated Reddit posts labelled as \"Stress\" or \"Non-Stress\". These posts contain informal language, indirect expressions, and domain specific discourse patterns. This makes stress detection both linguistically complex and context dependent.\n",
    "\n",
    "The objective of this assignment is to build a binary classification model capable of identifying stress in Reddit text while emphasizing interpretability and sense making. Beyond predictive performance, this study investigates which lexical, syntactic, and social signals contribute to stress detection, how model errors manifest across communities, and what ethical considerations arise in potential real world deployment.\n",
    "\n",
    "By integrating psychologically grounded feature engineering with text based modelling, this work aims to balance predictive accuracy with transparency and responsible application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a57a297",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preprocessing\n",
    "\n",
    "Prior to modelling, we apply structured preprocessing to ensure data integrity, reproducibility, and deployment realism.\n",
    "The following subsections detail text normalization, metadata removal, and feature organization decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04f372",
   "metadata": {},
   "source": [
    "### 2.1 Importing of Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ed9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "import shap\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c0bf9a",
   "metadata": {},
   "source": [
    "### 2.2 Data Loading\n",
    "\n",
    "We use the predefined Dreaddit training and test splits to ensure fair evaluation and prevent data leakage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"data/dreaddit-train.csv\")\n",
    "\n",
    "print(train_raw.shape)\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514adc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw  = pd.read_csv(\"data/dreaddit-test.csv\")\n",
    "\n",
    "print(test_raw.shape)\n",
    "test_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795baaa",
   "metadata": {},
   "source": [
    "### 2.3 Data Integrity Checks\n",
    "\n",
    "No missing values or duplicate entries were detected in either training or test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d535e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train missing:\\n\", train_raw.isna().sum().sum())\n",
    "print(\"Test missing:\\n\", test_raw.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b25d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train duplicates:\", train_raw.duplicated().sum())\n",
    "print(\"Test duplicates:\", test_raw.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3838c597",
   "metadata": {},
   "source": [
    "### 2.4 Minimal Text Cleaning\n",
    "\n",
    "We apply light preprocessing by:\n",
    "- Removing rows with missing text or labels\n",
    "- Ensuring labels are binary integers\n",
    "- Removing URLs\n",
    "- Trimming whitespace\n",
    "\n",
    "We deliberately avoid aggressive normalization (e.g., stopword removal or lemmatization), as stress detection is sensitive to function words and self referential cues that may be removed by heavy preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eac3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"http\\S+|www\\S+\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def basic_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(subset=[\"text\", \"label\"]).copy()\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    df[\"text\"] = df[\"text\"].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "train_df = basic_clean(train_raw)\n",
    "test_df  = basic_clean(test_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9999a324",
   "metadata": {},
   "source": [
    "### 2.5 Removing Non Generalizable Columns\n",
    "\n",
    "Here, we remove identifier and annotation metadata columns (e.g., post_id, id, confidence, sentence_range). This prevents information leakage and ensures the model relies only on features available in real world deployment setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff76453",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"id\", \"post_id\", \"sentence_range\", \"confidence\"]\n",
    "train_df = train_df.drop(columns=[c for c in drop_cols if c in train_df.columns])\n",
    "test_df  = test_df.drop(columns=[c for c in drop_cols if c in test_df.columns])\n",
    "\n",
    "print(\"Train cleaned:\", train_df.shape)\n",
    "print(\"Test cleaned :\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdda22b",
   "metadata": {},
   "source": [
    "### 2.6 Feature Group Identification\n",
    "\n",
    "Following the Dreaddit feature taxonomy, we group numeric features into:\n",
    "\n",
    "* Lexical features (LIWC, DAL, sentiment)\n",
    "* Syntactic features (readability indices)\n",
    "* Social media features (engagement related metrics)\n",
    "\n",
    "This grouping allows systematic comparison of feature subsets during modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5416d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_cols = [c for c in train_df.columns if c.startswith(\"lex_\")]\n",
    "syn_cols = [c for c in train_df.columns if c.startswith(\"syntax_\")]\n",
    "soc_cols = [c for c in train_df.columns if c.startswith(\"social_\")]\n",
    "\n",
    "print(\"Lexical features:\", len(lex_cols))\n",
    "print(\"Syntactic features:\", len(syn_cols))\n",
    "print(\"Social features:\", len(soc_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9144f37",
   "metadata": {},
   "source": [
    "Lexical features dominates with 102 variables, compared to 2 syntactic and 4 social features. This imbalance motivates later dimensionality reduction to mitigate overfitting from high dimensional lexical signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f4a79",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "We conduct exploratory analysis on the training data to understand patterns associated with stress.\n",
    "\n",
    "This section aims to:\n",
    "\n",
    "1. Examine dataset balance.\n",
    "2. Explore stress prevalence across subreddits.\n",
    "3. Investigate narrative length differences.\n",
    "4. Identify lexical and linguistic signals associated with stress.\n",
    "5. Motivate feature selection and modelling choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cec157",
   "metadata": {},
   "source": [
    "### 3.1 Distribution of Stress vs Non-Stress Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647db5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = train_df[\"label\"].value_counts().sort_index()\n",
    "labels = [\"Non-Stress\", \"Stress\"]\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "bars = plt.bar(labels, label_counts.values, color=sns.color_palette(\"viridis\", 2))\n",
    "\n",
    "for i, v in enumerate(label_counts.values):\n",
    "    plt.text(i, v + 10, f\"{v}\\n({v/len(train_df)*100:.1f}%)\", ha='center')\n",
    "\n",
    "plt.title(\"Distribution of Stress vs Non-Stress Posts in Training Data\")\n",
    "plt.ylim(0, max(label_counts.values) + 200)\n",
    "plt.ylabel(\"Number of Posts\", fontsize=12)\n",
    "plt.xlabel(\"Label\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aed9f7",
   "metadata": {},
   "source": [
    "The dataset is approximately balanced between stressed and non-stressed posts. This supports the use of F1 score as the primary evaluation metric, as neither class overwhelmingly dominates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a98fcaa",
   "metadata": {},
   "source": [
    "### 3.2 Stress Prevalence Across Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3effc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_rate = (train_df.groupby(\"subreddit\")[\"label\"].mean() * 100).sort_values()\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "cmap = plt.get_cmap('viridis')\n",
    "bars = plt.barh(\n",
    "    stress_rate.index,\n",
    "    stress_rate.values,\n",
    "    color=cmap(stress_rate.values / stress_rate.values.max())\n",
    ")\n",
    "\n",
    "for i, v in enumerate(stress_rate.values):\n",
    "    plt.text(v + 1, i, f\"{v:.1f}%\", va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "\n",
    "plt.title(\"Stress Prevalence by Subreddit\")\n",
    "plt.xlabel(\"Proportion of Posts Labeled as Stress\", fontsize=12)\n",
    "plt.ylabel(\"Subreddit\", fontsize=12)\n",
    "plt.xlim(0, 100) # Percentage scale\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843854da",
   "metadata": {},
   "source": [
    "Stress rate is computed as the mean of the binary stress label within each subreddit. The Viridis gradient encodes increasing prevalence, with lighter bars indicating higher proportions of stress labeled posts.\n",
    "\n",
    "Stress prevalence varies substantially across subreddit communities. Mental health–focused subreddits exhibit higher baseline stress rates, while general forums show lower levels. This heterogeneity suggests that stress expression is domain dependent and that classification performance may differ across contexts. Consequently, this necessitates a model that can generalize across different emotional intensities instead of a 'one size fits all' threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b42395",
   "metadata": {},
   "source": [
    "### 3.3 Word Count Distribution by Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920627f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"Non-Stress\", 1: \"Stress\"}\n",
    "train_df[\"Condition\"] = train_df[\"label\"].map(label_map)\n",
    "\n",
    "train_df['word_count'] = train_df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.histplot(data=train_df, x='word_count', hue='Condition', kde=True, bins=50, \n",
    "             palette=[\"#440154\", \"#21918c\"])\n",
    "\n",
    "stress_avg = train_df[train_df['Condition'] == 'Stress']['word_count'].mean()\n",
    "non_stress_avg = train_df[train_df['Condition'] == 'Non-Stress']['word_count'].mean()\n",
    "\n",
    "plt.axvline(stress_avg, color='#440154', linestyle='--', alpha=0.8, \n",
    "            label=f'Stress Avg: {stress_avg:.0f} words')\n",
    "plt.axvline(non_stress_avg, color='#21918c', linestyle='--', alpha=0.8, \n",
    "            label=f'Non-Stress Avg: {non_stress_avg:.0f} words')\n",
    "\n",
    "plt.title('Distribution of Word Count in Stress vs Non-Stress Posts')\n",
    "plt.xlabel('Number of Words', fontsize=12)\n",
    "plt.ylabel('Number of Posts', fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664acd83",
   "metadata": {},
   "source": [
    "Stress posts are marginally longer on average and show a slightly heavier right tail. However, the distributions overlap substantially, indicating that text length alone is not a strong discriminator. This suggests that lexical content rather than length drives predictive signal, supporting the use of TF-IDF representations in subsequent modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a9091",
   "metadata": {},
   "source": [
    "### 3.4 Exploratory Linguistic Signals\n",
    "\n",
    "We visualize selected representative linguistic markers across psychological dimensions to explore potential differences between stress and non-stress posts. These features were selected for interpretability and theoretical relevance, allowing us to qualitatively assess how stressed and non-stressed posts differ before formal modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a602a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_mapping = {\n",
    "    \"lex_liwc_negemo\": \"Negative Emotion\",\n",
    "    \"lex_liwc_posemo\": \"Positive Emotion\",\n",
    "    \"lex_liwc_anx\": \"Anxiety Terms\",\n",
    "    \"lex_liwc_Tone\": \"Emotional Tone\",\n",
    "    \"lex_liwc_Clout\": \"Clout\",\n",
    "    \"lex_liwc_i\": \"First Person (I)\",\n",
    "    \"lex_liwc_you\": \"Second Person (You)\",\n",
    "    \"lex_liwc_social\": \"Social Words\",\n",
    "    \"lex_liwc_Authentic\": \"Authenticity\",\n",
    "    \"lex_liwc_Analytic\": \"Analytical Thinking\",\n",
    "    \"lex_liwc_feel\": \"Feeling Words\",\n",
    "    \"lex_liwc_function\": \"Function Words\",\n",
    "    \"lex_liwc_Dic\": \"Dictionary Coverage\",\n",
    "    \"lex_liwc_Apostro\": \"Apostrophes\",\n",
    "    \"lex_dal_min_pleasantness\": \"Min. Pleasantness\",\n",
    "    \"lex_dal_avg_pleasantness\": \"Avg. Pleasantness\",\n",
    "    \"lex_dal_avg_activation\": \"Avg. Activation\",\n",
    "    \"syntax_fk_grade\": \"Flesch-Kincaid Grade\",\n",
    "    \"syntax_ari\": \"Automated Readability Index\",\n",
    "    \"sentiment\": \"Sentiment\",\n",
    "    \"social_karma\": \"Social Karma\",\n",
    "    \"social_num_comments\": \"Social Comment Count\",\n",
    "    \"social_upvote_ratio\": \"Social Upvote Ratio\",\n",
    "    \"label\": \"Stress Label (1=Stress)\"\n",
    "}\n",
    "\n",
    "def map_name(name):\n",
    "    return name_mapping.get(name, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_features = [\"lex_liwc_negemo\", \"lex_liwc_i\", \"lex_liwc_Clout\", \"sentiment\"]\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(12,8))\n",
    "\n",
    "for ax, feature in zip(axes.flatten(), key_features):\n",
    "    sns.boxplot(data=train_df, x=\"Condition\", y=feature, ax=ax, palette=\"viridis\", hue=\"Condition\")\n",
    "    ax.set_title(map_name(feature))\n",
    "    ax.set_xlabel(\"Condition\")\n",
    "    ax.set_ylabel(map_name(feature))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe334f96",
   "metadata": {},
   "source": [
    "From the boxplots, several clear patterns emerge:\n",
    "\n",
    "* Negative emotion is higher in stressed posts.\n",
    "* First person pronoun usage (I) increases under stress, suggesting greater self focus.\n",
    "* Clout decreases in stressed posts, indicating that users in distress express themselves with less confidence or social certainty.\n",
    "* Sentiment shifts more negatively. \n",
    "\n",
    "While distributions overlap, the median differences support the model’s reliance on emotional and self referential language features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58928824",
   "metadata": {},
   "source": [
    "### 3.5 Correlation Heatmap of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = [\n",
    "    \"lex_liwc_negemo\",\n",
    "    \"lex_liwc_posemo\",\n",
    "    \"lex_liwc_anx\",\n",
    "    \"lex_liwc_i\",\n",
    "    \"lex_liwc_Clout\",\n",
    "    \"lex_liwc_Tone\",\n",
    "    \"syntax_fk_grade\",\n",
    "    \"syntax_ari\",\n",
    "    \"sentiment\",\n",
    "    \"label\",\n",
    "]\n",
    "\n",
    "corr = train_df[selected_cols].corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "ax = sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap=\"viridis\",          \n",
    "    vmin=-1, vmax=1,\n",
    "    center=0,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",            \n",
    "    linewidths=0.5,\n",
    "    linecolor=\"white\",\n",
    "    cbar_kws={\"label\": \"Pearson correlation (r)\"},\n",
    ")\n",
    "\n",
    "ax.set_xticklabels([map_name(c) for c in corr.columns], rotation=35, ha=\"right\")\n",
    "ax.set_yticklabels([map_name(c) for c in corr.index], rotation=0)\n",
    "\n",
    "plt.title(\"Correlation Heatmap of Selected Psychological Markers\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1f9fe",
   "metadata": {},
   "source": [
    "Stress is positively correlated with negative emotion, anxiety related terms, and first person pronouns, indicating greater emotional negativity and self focus in stressed posts. Conversely, stress is negatively correlated with Tone, Clout, and overall sentiment, reflecting reduced positivity and linguistic confidence.\n",
    "\n",
    "The heatmap also reveals multicollinearity among several predictors. Flesch–Kincaid Grade and ARI are highly correlated (r ≈ 0.97), suggesting redundancy, and multiple LIWC features exhibit moderate intercorrelation. Although readability features show weaker direct association with stress, they capture structural aspects of expression that complement lexical signals. Correlation based feature selection is therefore applied in the modelling phase to reduce redundancy and improve stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48177dc8",
   "metadata": {},
   "source": [
    "## 4. Feature Selection and Model Preparation\n",
    "\n",
    "The Dreaddit dataset provides high dimensional lexical, syntactic, and social features in addition to raw text. Building on exploratory findings, including all available predictors risks introducing redundancy and noise, particularly given the dominance of lexical variables. Correlation based filtering is applied on the training subset to preserve deployment practicality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298f33d",
   "metadata": {},
   "source": [
    "### 4.1 Train–Validation Split\n",
    "\n",
    "To maintain statistical validity and prevent information leakage, the training data is split into training and validation subsets prior to feature selection. All selection and tuning decisions are based exclusively on the training subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16001300",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = \"text\"\n",
    "target_col = \"label\"\n",
    "numeric_cols = lex_cols + syn_cols + soc_cols\n",
    "\n",
    "X = train_df.drop(columns=[target_col])\n",
    "y = train_df[target_col]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Total numeric features:\", len(numeric_cols))\n",
    "print(\"Training subset:\", X_train.shape)\n",
    "print(\"Validation subset:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c13c6a3",
   "metadata": {},
   "source": [
    "### 4.2 Correlation Based Feature Selection\n",
    "\n",
    "To reduce dimensionality and improve interpretability, Pearson correlation between each numeric feature and the stress label is computed on the training subset. Absolute correlation values are used to identify features with meaningful linear association.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88efd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corr_df = X_train[numeric_cols].copy()\n",
    "train_corr_df[target_col] = y_train.values\n",
    "\n",
    "corr_to_label = train_corr_df.corr(numeric_only=True)[target_col].drop(target_col)\n",
    "\n",
    "abs_corr = corr_to_label.abs().sort_values(ascending=False)\n",
    "top_corr = abs_corr.sort_values(ascending=False).head(10)\n",
    "\n",
    "colors = plt.cm.viridis(top_corr.values / top_corr.values.max())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    x=top_corr.values,\n",
    "    y=[map_name(c) for c in top_corr.index],\n",
    "    hue=[map_name(c) for c in top_corr.index], \n",
    "    palette=\"viridis_r\",                      \n",
    "    legend=False,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "for i, v in enumerate(top_corr.values):\n",
    "    plt.text(v + 0.002, i, f\"{v:.3f}\", va='center')\n",
    "\n",
    "ax.set_title(\"Top Numeric Features Correlated with Stress\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Absolute Pearson Correlation coefficient\", fontsize=12)\n",
    "ax.set_ylabel(\"Feature\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4bf4b9",
   "metadata": {},
   "source": [
    "The figure shows the strongest predictors ranked by absolute Pearson correlation. Lexical emotional markers dominate the top associations, with Tone, Clout, and first person pronouns exhibiting the highest correlation with stress.\n",
    "\n",
    "Multiple thresholds (|r| ≥ 0.1–0.4) were evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0992831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [0.1, 0.2, 0.3, 0.4]:\n",
    "    print(f\"|r| ≥ {t}: {(abs_corr >= t).sum()} features selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c18593",
   "metadata": {},
   "source": [
    "A threshold of |r| ≥ 0.2 is chosen as it retained 15 features, balancing dimensionality reduction with signal preservation. It is important to note that Pearson correlation captures only linear relationships between predictors and the stress label. More complex non linear feature selection techniques could potentially identify additional signals. However, a correlation based filter was chosen to preserve interpretability, reduce dimensionality transparently, and align feature selection with theoretically grounded linguistic markers identified during exploratory analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "\n",
    "selected_features = abs_corr[abs_corr >= threshold].index.tolist()\n",
    "\n",
    "print(\"Selected features:\", len(selected_features))\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a36b741",
   "metadata": {},
   "source": [
    "### 4.3 Multicollinearity Reduction\n",
    "\n",
    "Pairwise correlations among the retained features were then examined to mitigate redundancy (|r| > 0.9). No feature pairs exceeded this threshold, indicating that the filtering step effectively produced a complementary and a non-collinear predictor set. Although readability indices were highly correlated in exploratory analysis, they did not surpass the label correlation threshold and were excluded prior to multicollinearity screening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89779221",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = X_train[selected_features]\n",
    "\n",
    "corr_matrix = selected_df.corr().abs()\n",
    "\n",
    "upper_triangle = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "high_corr = [\n",
    "    column for column in upper_triangle.columns\n",
    "    if any(upper_triangle[column] > 0.9)\n",
    "]\n",
    "\n",
    "print(\"Highly correlated features to remove:\", high_corr)\n",
    "\n",
    "final_selected_features = [\n",
    "    f for f in selected_features\n",
    "    if f not in high_corr\n",
    "]\n",
    "\n",
    "print(\"Final feature count:\", len(final_selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93ccec",
   "metadata": {},
   "source": [
    "### 4.4 Final Feature Configurations for Modelling\n",
    "\n",
    "We define three modelling configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e3c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_features = None\n",
    "text_plus_selected = final_selected_features\n",
    "text_plus_all = numeric_cols\n",
    "\n",
    "print(\"Text only model: uses TF-IDF from raw text only\")\n",
    "print(\"Text + Selected features:\", len(text_plus_selected))\n",
    "print(\"Text + All numeric features:\", len(text_plus_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd606ab",
   "metadata": {},
   "source": [
    "This structured comparison allows us to quantify the incremental contribution of engineered features beyond raw text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b62ab1",
   "metadata": {},
   "source": [
    "## 5. Modelling Pipeline and Model Comparison\n",
    "\n",
    "Following feature construction and selection in Section 4, we evaluate whether these representations enable reliable stress classification. \n",
    "\n",
    "A naive baseline is first established using a Dummy Classifier to determine minimum achievable performance without learning.\n",
    "We then compare 3 common classifiers for text classification:\n",
    "\n",
    "- **Logistic Regression**: linear model with interpretability.\n",
    "- **Linear SVM**: strong performer for high dimensional sparse text.\n",
    "- **Random Forest**: non-linear ensemble.\n",
    "\n",
    "Models are trained on the training subset, selected based on validation F1 score, and evaluated on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d90a416",
   "metadata": {},
   "source": [
    "### 5.1 Dummy Classifier\n",
    "\n",
    "The baseline classifier predicts the majority class, providing a reference point to ensure our model captures genuine predictive signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1077a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COL = \"text\"\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train[TEXT_COL], y_train)\n",
    "\n",
    "dummy_pred = dummy.predict(X_val[TEXT_COL])\n",
    "baseline_f1 = f1_score(y_val, dummy_pred)\n",
    "\n",
    "print(f\"Baseline F1: {baseline_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d808938",
   "metadata": {},
   "source": [
    "### 5.2 Modelling Pipeline\n",
    "\n",
    "To ensure consistency across models, a unified pipeline is constructed. Text features are transformed using TF-IDF, optionally combined with structured numeric features, and passed to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dense(X):\n",
    "    return X.toarray() if hasattr(X, \"toarray\") else X\n",
    "\n",
    "def build_linear_pipeline(model, numeric_features=None, tfidf_max_features=5000):\n",
    "    transformers = [\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2),\n",
    "                                  min_df=5,\n",
    "                                  max_features=tfidf_max_features),\n",
    "         TEXT_COL)\n",
    "    ]\n",
    "    \n",
    "    if numeric_features is not None and len(numeric_features) > 0:\n",
    "        transformers.append((\"num\", StandardScaler(), numeric_features))\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers, remainder=\"drop\")\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "    return pipe\n",
    "\n",
    "def build_rf_pipeline(numeric_features=None, tfidf_max_features=5000, random_state=42):\n",
    "    transformers = [\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2),\n",
    "                                  min_df=5,\n",
    "                                  max_features=tfidf_max_features),\n",
    "         TEXT_COL)\n",
    "    ]\n",
    "    if numeric_features is not None and len(numeric_features) > 0:\n",
    "        transformers.append((\"num\", StandardScaler(), numeric_features))\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers, remainder=\"drop\")\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"to_dense\", FunctionTransformer(to_dense, accept_sparse=True)),\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            max_depth=None,\n",
    "            n_jobs=-1,\n",
    "            random_state=random_state\n",
    "        ))\n",
    "    ])\n",
    "    return pipe\n",
    "\n",
    "def evaluate_model(pipe):\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred = pipe.predict(X_val)\n",
    "    return f1_score(y_val, pred), pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc590192",
   "metadata": {},
   "source": [
    "### 5.3 Model Comparison on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4316c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42),\n",
    "    \"Linear SVM\": LinearSVC(max_iter=5000, random_state=42),\n",
    "}\n",
    "\n",
    "feature_sets = {\n",
    "    \"Text Only\": [],\n",
    "    \"Text + Selected Numeric\": final_selected_features,\n",
    "    \"Text + All Numeric\": numeric_cols\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for feat_name, num_feats in feature_sets.items():\n",
    "    for model_name, model in models.items():\n",
    "        pipe = build_linear_pipeline(model, num_feats, tfidf_max_features=5000)\n",
    "        f1, _ = evaluate_model(pipe)\n",
    "        results.append({\"Model\": model_name, \"Features\": feat_name, \"Val F1\": f1})\n",
    "    \n",
    "    rf_pipe = build_rf_pipeline(num_feats, tfidf_max_features=5000, random_state=42)\n",
    "    f1, _ = evaluate_model(rf_pipe)\n",
    "    results.append({\"Model\": \"Random Forest\", \"Features\": feat_name, \"Val F1\": f1})\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Val F1\", ascending=False).reset_index(drop=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c9990",
   "metadata": {},
   "source": [
    "Logistic Regression with Text + Selected Numeric features achieved the highest validation F1 score at ~0.792, outperforming Linear SVM and Random Forest across configurations.\n",
    "\n",
    "Several patterns emerge:\n",
    "\n",
    "1. Linear models outperform Random Forest: Stress classification in high dimensional TF-IDF space appears largely linearly separable. The non-linear capacity of Random Forest does not provide additional benefit and may introduce variance.\n",
    "\n",
    "2. Selected numeric features outperform using all numeric features: Performance slightly drops when all engineered features are included (0.785 vs 0.792), suggesting that weaker or noisy predictors may dilute signal. Correlation-based filtering improves stability and reduces overfitting.\n",
    "\n",
    "3. Feature augmentation improves over text only models: This indicates that structured psychological markers (e.g., negative emotion, self focus) provide incremental predictive value beyond lexical content alone.\n",
    "\n",
    "Overall, Logistic Regression with selected engineered features provides the best balance between performance, interpretability, and model simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b873c6f",
   "metadata": {},
   "source": [
    "### 5.4 Cross Validation on Best Model\n",
    "\n",
    "To evaluate robustness, five-fold stratified cross validation was performed on the best model: Logistic Regression with Text + Selected Numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983abbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe = build_linear_pipeline(\n",
    "    LogisticRegression(max_iter=2000, random_state=42),\n",
    "    final_selected_features\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = cross_val_score(best_pipe, X_train, y_train, cv=cv, scoring=\"f1\")\n",
    "\n",
    "print(f\"5-fold CV F1 mean: {cv_scores.mean():.3f}\")\n",
    "print(f\"5-fold CV F1 std : {cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc433e",
   "metadata": {},
   "source": [
    "Cross validation results demonstrate stable performance across folds, indicating that the selected feature configuration generalizes consistently within the training distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0d67f",
   "metadata": {},
   "source": [
    "### 5.5 Hyperparameter Tuning\n",
    "\n",
    "Logistic Regression includes a regularization parameter C, which controls the trade-off between model complexity and generalization. \n",
    "\n",
    "* Smaller C -> stronger regularization -> simpler decision boundary  \n",
    "* Larger C -> weaker regularization -> more complex boundary  \n",
    "\n",
    "In high-dimensional TF-IDF space, appropriate regularization is critical to prevent overfitting. We therefore perform grid search over multiple C values and select the model with the highest cross-validated F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2856b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_pipe = build_linear_pipeline(\n",
    "    LogisticRegression(max_iter=2000, random_state=42),\n",
    "    final_selected_features\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"clf__C\": [0.1, 0.5, 1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    logreg_pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV F1:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b17c6",
   "metadata": {},
   "source": [
    "Grid search identified C = 2 as the optimal regularization strength, achieving a 5-fold cross validated F1 score of 0.785. \n",
    "\n",
    "Performance differences across C values were modest, suggesting that the model is relatively stable with respect to regularization strength. This indicates that predictive performance is driven primarily by feature representation rather than fine tuning of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb19044",
   "metadata": {},
   "source": [
    "### 5.6 Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88489ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.drop(columns=[\"label\"])\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "final_model = grid.best_estimator_\n",
    "\n",
    "test_pred = final_model.predict(X_test)\n",
    "\n",
    "print(\"Test F1:\", f1_score(y_test, test_pred))\n",
    "print(classification_report(y_test, test_pred, target_names=[\"Non-Stress\", \"Stress\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4945e",
   "metadata": {},
   "source": [
    "The tuned Logistic Regression model achieves a test F1 score of 0.777 on the held out dataset, confirming strong generalization beyond validation data.\n",
    "\n",
    "Recall for stress cases (0.81) exceeds precision (0.75), indicating that the model prioritizes identifying distressed individuals while accepting moderate false positives. In mental health screening contexts, this trade-off is often desirable, as failing to detect genuinely stressed individuals may carry greater risk than overflagging.\n",
    "\n",
    "However, summary metrics do not reveal where errors occur or how performance varies across communities. These issues are examined in Section 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51723437",
   "metadata": {},
   "source": [
    "## 6. Analysis of Model Beyond Predictive Performance\n",
    "\n",
    "While the overall test F1 score (≈ 0.777) indicates strong predictive performance, it does not explain how stress is expressed, where the model struggles, or what risks may arise in deployment.\n",
    "\n",
    "To satisfy the “sense making” objective of this assignment, we move beyond aggregate metrics and analyse:\n",
    "\n",
    "1. Model Behaviour\n",
    "2. Performance variation across subreddits  \n",
    "3. Failure patterns (false negatives and false positives)  \n",
    "4. Structural factors such as post length  \n",
    "5. Sensitivity to decision thresholds\n",
    "6. Ethical considerations and deployment risks\n",
    "\n",
    "Together, these analyses clarify both the strengths and limitations of the model in real world settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14440ab2",
   "metadata": {},
   "source": [
    "### 6.1 Model Behaviour\n",
    "\n",
    "To interpret how the model makes predictions, SHAP (SHapley Additive exPlanations) values were computed on the test set. SHAP quantifies the contribution of each feature to the predicted probability of stress for individual samples by assigning each feature a contribution value for each post: values to the right increase the predicted probability of Stress, while values to the left push the prediction toward Non-Stress. Features are ranked by overall impact (mean absolute SHAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75562383",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid.best_estimator_\n",
    "\n",
    "test_pred = final_model.predict(X_test)\n",
    "test_probs = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_df_analysis = X_test.copy()\n",
    "test_df_analysis[\"true_label\"] = y_test.values\n",
    "test_df_analysis[\"pred_label\"] = test_pred\n",
    "test_df_analysis[\"prob_stress\"] = test_probs\n",
    "\n",
    "print(test_df_analysis.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb650d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_feature(name: str) -> tuple[str, str]:\n",
    "    if name.startswith(\"num__\"):\n",
    "        raw = name.replace(\"num__\", \"\")\n",
    "        raw = raw.replace(\"lex_liwc_\", \"\")\n",
    "        raw = raw.replace(\"lex_dal_\", \"\")\n",
    "        raw = raw.replace(\"_\", \" \").strip()\n",
    "\n",
    "        fixes = {\n",
    "            \"negemo\": \"Negative Emotion\",\n",
    "            \"posemo\": \"Positive Emotion\",\n",
    "            \"anx\": \"Anxiety\",\n",
    "            \"i\": \"First-Person (I)\",\n",
    "            \"clout\": \"Clout\",\n",
    "            \"tone\": \"Emotional Tone\",\n",
    "            \"apostro\": \"Apostrophes\",\n",
    "            \"dic\": \"Dictionary words\",\n",
    "        }\n",
    "        key = raw.lower().replace(\" \", \"\")\n",
    "        pretty = fixes.get(key, raw.title())\n",
    "\n",
    "        return f\"{pretty} (LIWC/DAL)\", \"LIWC/DAL\"\n",
    "\n",
    "    if name.startswith(\"tfidf__\"):\n",
    "        token = name.replace(\"tfidf__\", \"\").strip()\n",
    "        return f\"{token} (TF-IDF)\", \"TF-IDF\"\n",
    "\n",
    "    return name, \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f5286",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_trans = final_model.named_steps[\"preprocess\"].transform(X_test)\n",
    "feature_names = final_model.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "\n",
    "pretty_names = [pretty_feature(f)[0] for f in feature_names]\n",
    "\n",
    "clf = final_model.named_steps[\"clf\"]\n",
    "explainer = shap.LinearExplainer(clf, X_test_trans, feature_names=pretty_names)\n",
    "shap_values = explainer(X_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8096484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shap = np.abs(shap_values.values).mean(axis=0)\n",
    "\n",
    "top_idx = np.argsort(mean_shap)[-15:]\n",
    "\n",
    "colors = cm.viridis(\n",
    "    mean_shap[top_idx] / mean_shap[top_idx].max()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.barh(\n",
    "    [feature_names[i] for i in top_idx],\n",
    "    mean_shap[top_idx],\n",
    "    color=colors\n",
    ")\n",
    "\n",
    "plt.gca()\n",
    "plt.xlabel(\"Mean |SHAP Value|\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.title(\"Global Feature Importance\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382d17c",
   "metadata": {},
   "source": [
    "The global SHAP importance ranking indicates that engineered linguistic markers dominate model behaviour. Emotional tone exhibits the largest average contribution, followed by first person pronouns and negative emotion.\n",
    "\n",
    "This confirms that stress classification is primarily driven by psychological expression patterns rather than isolated keywords. Structural markers such as clout and pleasantness also contribute meaningfully, suggesting that both affective polarity and linguistic confidence are central signals.\n",
    "\n",
    "In contrast, individual TF-IDF tokens exhibit smaller average impact, indicating that lexical cues operate within broader emotional patterns rather than independently driving predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7dca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    X_test_trans,\n",
    "    feature_names=feature_names,\n",
    "    max_display=15,\n",
    "    cmap=\"viridis\", \n",
    "    show=False)\n",
    "\n",
    "plt.title(\"Top Drivers of Stress Prediction\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4ee29",
   "metadata": {},
   "source": [
    "While the barplot ranks feature importance globally, the beeswarm plot further reveals directional effects. The most influential predictors are engineered linguistic markers (LIWC/DAL). Features pushing predictions toward stress include:\n",
    "* Lower emotional tone,\n",
    "* Higher frequency of first person pronouns (“I”),\n",
    "* Increased negative emotion terms,\n",
    "* Lower linguistic clout (reduced confidence).\n",
    "\n",
    "These patterns are consistent with Section 3.5 (correlation heatmap) and Section 4 (feature selection). The SHAP results therefore validate that the final model operationalises the same psychological signals identified during exploratory analysis.\n",
    "\n",
    "Conversely, higher tone and greater linguistic confidence shift predictions toward non-stress, suggesting that emotionally positive and assertive language reduces the likelihood of stress classification.\n",
    "\n",
    "Although engineered linguistic markers dominate the top positions, several TF-IDF tokens also appear among the most influential features. Many of these words occur within common constructions such as “I can’t even…” or “I don’t know what to do,” which convey overwhelm or perceived lack of control. These phrases act as contextual signals of distress and not as standalone keywords. Their presence reflects how the model captures specific linguistic patterns associated with stress expression.\n",
    "\n",
    "Importantly, engineered linguistic features display broader and more consistent impact across samples than individual TF-IDF tokens. While tokens may spike importance for certain posts, psychological markers such as tone, clout, and negative affect contribute systematically across the dataset. This suggests that stress prediction is driven primarily by patterns of emotional expression and self-focus, rather than reliance on a small set of trigger words.\n",
    "\n",
    "As the model is linear, coefficients provide global interpretability, while SHAP provides instance level explanations. The model is therefore transparent at both aggregate and individual levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28819db8",
   "metadata": {},
   "source": [
    "### 6.2 Subreddit Level Performance\n",
    "\n",
    "We evaluate subreddit level F1 to examine whether certain forms of stress are easier to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fd5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_stats = (\n",
    "    test_df_analysis\n",
    "    .groupby(\"subreddit\")\n",
    "    .apply(lambda x: pd.Series({\n",
    "        \"n\": len(x),\n",
    "        \"F1\": f1_score(x[\"true_label\"], x[\"pred_label\"])\n",
    "    }))\n",
    "    .sort_values(\"F1\", ascending=False)\n",
    ")\n",
    "\n",
    "display(subreddit_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4943b",
   "metadata": {},
   "source": [
    "Performance varies substantially across communities. Mental health–focused subreddits (e.g., ptsd, domesticviolence) show higher F1 scores. Whereas, relationship oriented or general discussion forums show lower performance, likely because posts contain mixed emotional signals (conflict, reconciliation, ambiguity) that blur the boundary between stress and non-stress. \n",
    "\n",
    "This suggests stress is expressed more explicitly in mental health communities and more indirectly in general domains. The variation indicates that stress is context dependent, and a single global model may not perform uniformly across communities. This domain dependence has implications for generalisation and fairness.\n",
    "\n",
    "Performance estimates for subreddits with small sample sizes (e.g., n < 20) should be interpreted cautiously, as evaluation metrics may exhibit high variance under limited observations. Consequently, apparent performance differences in smaller communities may not generalise reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79727d14",
   "metadata": {},
   "source": [
    "### 6.3 Length Based Performance\n",
    "\n",
    "To assess whether post length influences performance, F1 score was computed across length bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbaad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_analysis[\"word_count\"] = test_df_analysis[\"text\"].astype(str).apply(lambda x: len(x.split()))\n",
    "test_df_analysis[\"length_bin\"] = pd.qcut(test_df_analysis[\"word_count\"], q=4)\n",
    "\n",
    "length_perf = (\n",
    "    test_df_analysis\n",
    "    .groupby(\"length_bin\")\n",
    "    .apply(lambda x: f1_score(x[\"true_label\"], x[\"pred_label\"]))\n",
    ")\n",
    "\n",
    "length_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d8151",
   "metadata": {},
   "source": [
    "Performance remains relatively stable across most bins, with only minor variation. Both stressed and non-stressed posts exhibit overlapping length distributions, indicating that length alone is not a strong discriminative signal.\n",
    "\n",
    "This suggests the model does not rely heavily on verbosity as a proxy for distress. Instead, classification appears driven primarily by linguistic content rather than structural length differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11211f",
   "metadata": {},
   "source": [
    "### 6.4 Threshold Sensitivity\n",
    "\n",
    "By default, logistic regression uses a probability threshold of 0.5 to classify stress. However, in mental health applications, the cost of false negatives may outweigh false positives.\n",
    "\n",
    "We therefore examine how precision and recall change under different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648baecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df_analysis[\"true_label\"].astype(int).values\n",
    "X_test = test_df_analysis.drop(columns=[\"true_label\"])\n",
    "\n",
    "probs = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "thresholds = np.arange(0.30, 0.71, 0.05)  \n",
    "rows = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (probs >= t).astype(int)\n",
    "\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    rows.append({\n",
    "        \"Threshold\": round(float(t), 2),\n",
    "        \"F1\": f1,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"FP\": int(fp),\n",
    "        \"FN\": int(fn),\n",
    "        \"TP\": int(tp),\n",
    "        \"TN\": int(tn),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "df_display = df.copy()\n",
    "for col in [\"Precision\", \"Recall\", \"F1\"]:\n",
    "    df_display[col] = df_display[col].round(4)\n",
    "\n",
    "display(df_display)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"Threshold\"], df[\"Precision\"], marker=\"o\", label=\"Precision\")\n",
    "plt.plot(df[\"Threshold\"], df[\"Recall\"], marker=\"o\", label=\"Recall\")\n",
    "plt.plot(df[\"Threshold\"], df[\"F1\"], marker=\"o\", label=\"F1\")\n",
    "\n",
    "plt.xlabel(\"Decision threshold for Stress\", fontsize=12)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.title(\"Threshold trade-off in Test set\", fontsize=14)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81fb25f",
   "metadata": {},
   "source": [
    "Adjusting the decision threshold systematically alters the precision–recall balance.\n",
    "\n",
    "* Lower thresholds (0.30–0.35) produce very high recall (0.94–0.91), meaning most stressed posts are detected. However, this comes at the cost of many false positives (FP = 175 at t=0.30).\n",
    "\n",
    "* The default threshold (0.50) provides a more balanced profile (Precision = 0.75, Recall = 0.81, F1 = 0.78), reducing false positives substantially while maintaining strong detection performance.\n",
    "\n",
    "* Higher thresholds (0.60–0.70) increase precision (up to 0.87 at t=0.70), meaning fewer false alarms, but recall drops sharply (down to 0.62), resulting in many missed stress cases (FN = 141 at t=0.70).\n",
    "\n",
    "F1 peaks around 0.35 (F1 ≈ 0.782) and remains relatively stable between 0.30 and 0.50, indicating that moderate thresholds maintain robust overall performance. Beyond 0.55, declining recall causes F1 to fall.\n",
    "\n",
    "This confirms the expected trade-off:\n",
    "\n",
    "* Lower thresholds prioritise case finding (high recall, more workload).\n",
    "\n",
    "* Higher thresholds prioritise precision (fewer alerts, higher risk of underdetection).\n",
    "\n",
    "In a mental health screening context, where missing distressed individuals carries asymmetric risk, a threshold slightly below the default (e.g., 0.35–0.40) may be defensible. However, if the model is deployed to triage limited human review capacity, the default threshold around 0.50 represents a pragmatic compromise between detection and operational burden.\n",
    "\n",
    "Threshold selection should therefore reflect the intended use case rather than default conventions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc7662",
   "metadata": {},
   "source": [
    "### 6.5 Probability Calibration Considerations\n",
    "\n",
    "While logistic regression produces probabilistic outputs, this study did not explicitly evaluate probability calibration. Calibration assesses whether predicted probabilities correspond to observed outcome frequencies, for example, whether posts predicted at 0.80 stress probability are stressed approximately 80% of the time.\n",
    "\n",
    "Because threshold selection relies on probability magnitudes, miscalibration could affect operational decisions in real world deployment. Future work may therefore incorporate calibration curves or reliability diagrams to verify probability accuracy, particularly if the model is used for triage or risk scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb144e7",
   "metadata": {},
   "source": [
    "### 6.6 Error Analysis \n",
    "\n",
    "To truly understand model limitations, we analyse both false negatives (missed stress) and false positives (incorrect stress flags), as well as subreddit differences, since they imply different deployment risks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2956d2b",
   "metadata": {},
   "source": [
    "#### 6.6.1 False Negatives & False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg = test_df_analysis[(test_df_analysis[\"true_label\"] == 1) & (test_df_analysis[\"pred_label\"] == 0)]\n",
    "false_pos = test_df_analysis[(test_df_analysis[\"true_label\"] == 0) & (test_df_analysis[\"pred_label\"] == 1)]\n",
    "\n",
    "cm = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"viridis\",\n",
    "    xticklabels=[\"Non-Stress\", \"Stress\"],\n",
    "    yticklabels=[\"Non-Stress\", \"Stress\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9322ece6",
   "metadata": {},
   "source": [
    "The confusion matrix shows that the model correctly identifies 298 stressed posts and 246 non-stressed posts. It achieves a recall of 0.81 for stress, indicating that most distressed posts are successfully detected. However, 71 stressed posts are missed (false negatives), representing underdetection risk in deployment settings. Additionally, 100 non-stressed posts are flagged as stress (false positives), reflecting overflagging.\n",
    "\n",
    "The model therefore exhibits recall oriented behaviour: it prioritises identifying stress at the cost of some false alarms. In triage settings, this trade-off may be acceptable if flagged cases are subject to human review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3f4b0",
   "metadata": {},
   "source": [
    "#### 6.6.2 Most Confident Wrong Predictions \n",
    "\n",
    "A granular examination of high confidence failures can reveal the systemic boundaries of language based stress detection. By analyzing \"Most Confident\" False Negatives (FN) and False Positives (FP), we can identify where computational signals decouple from psychological reality. These errors are not merely random noise but instead, they highlight two primary linguistic challenges:\n",
    "\n",
    "* Narrative Overpowering: Where a history of positive sentiment masks a current, subtle shift into crisis.\n",
    "* Affective Intensity Bias: Where high arousal, conflict oriented language is mistaken for active stress, regardless of the user's current state of resolution or reflection.\n",
    "\n",
    "The following case studies illustrate the tension between lexical frequency and contextual meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_example = false_neg.sort_values(\"prob_stress\").iloc[0]\n",
    "\n",
    "fp_example = false_pos.sort_values(\"prob_stress\", ascending=False).iloc[0]\n",
    "\n",
    "print(\"False Negative Example:\")\n",
    "print(\"Subreddit:\", fn_example[\"subreddit\"])\n",
    "print(\"Predicted Probability:\", round(fn_example[\"prob_stress\"], 3))\n",
    "print(\"True Label:\", fn_example[\"true_label\"])\n",
    "print(\"Predicted Label:\", fn_example[\"pred_label\"])\n",
    "print(\"\\nText:\\n\")\n",
    "print(fn_example[\"text\"])\n",
    "\n",
    "print(\"\\nFalse Positive Example:\")\n",
    "print(\"Subreddit:\", fp_example[\"subreddit\"])\n",
    "print(\"Predicted Probability:\", round(fp_example[\"prob_stress\"], 3))\n",
    "print(\"True Label:\", fp_example[\"true_label\"])\n",
    "print(\"Predicted Label:\", fp_example[\"pred_label\"])\n",
    "print(\"\\nText:\\n\")\n",
    "print(fp_example[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d960b4a",
   "metadata": {},
   "source": [
    "The False Negative example uses positive or neutral framing: \"met our true love,\" \"really happy,\" \"not NOT trying\". The only signal of distress is the final sentence: \"Things have been rough.\" As the majority of the post describes a happy timeline, the TF-IDF and LIWC positive emotion scores overwhelm the single \"rough\" marker. Hence, the model fails to understand the narrative arc and assigns very low stress probability despite contextual distress.\n",
    "\n",
    "This False Positive post is saturated with high intensity lexical markers: \"screaming,\" \"stupid,\" \"moody bitch,\" \"cheating,\" \"abusive asshole\". The LIWC negemo and anger scores for this post are likely in the 99th percentile. However, the user is speaking in the past tense (\"it took me months afterward,\" \"I remember\") and expressing agency/resolution (\"it was 100% in my power\"). The model cannot distinguish between active trauma (Stress) and processed trauma/reflection (Non-Stress).\n",
    "\n",
    "These high confidence errors suggest that the model's failures are structural. The False Negative in r/relationships suggests a need for a lower threshold to capture subtle, narrative driven distress. Conversely, the False Positive in r/survivorsofabuse highlights the risk of overtriage in trauma recovery communities, where high intensity language is used for healing rather than expressing active crisis. This reinforces that the model acts as a detector of intensity, not necessarily a diagnostic of clinical state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ffcf5",
   "metadata": {},
   "source": [
    "#### 6.6.3 Error Rates by Subreddit\n",
    "\n",
    "Error rates vary across communities, indicating domain dependence: some subreddits contain more ambiguous language where stress and non-stress are less separable. This motivates careful validation before transferring the model to new settings (e.g., Singapore specific platforms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7aa0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_errors = (\n",
    "    test_df_analysis\n",
    "    .groupby(\"subreddit\")\n",
    "    .apply(lambda x: pd.Series({\n",
    "        \"n\": len(x),\n",
    "        \"FN_rate\": ((x[\"true_label\"] == 1) & (x[\"pred_label\"] == 0)).mean(),\n",
    "        \"FP_rate\": ((x[\"true_label\"] == 0) & (x[\"pred_label\"] == 1)).mean(),\n",
    "    }))\n",
    "    .sort_values(\"FN_rate\", ascending=False)\n",
    ")\n",
    "\n",
    "subreddit_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a20a39",
   "metadata": {},
   "source": [
    "Examination of False Negatives and False Positives reveals systematic patterns, reflecting structural tendencies in how linguistic signals are weighted.\n",
    "\n",
    "* False Negatives (Stress → Non-Stress):\n",
    "These errors often occur when distress is implied through context rather than expressed with explicit negative emotion. Many examples use factual or restrained tone, suggesting the model is less sensitive to indirect or “stoic” stress expression. This is a high risk failure mode because stressed users may be missed.\n",
    "\n",
    "* False Positives (Non-Stress → Stress):\n",
    "These errors often contain strong emotion words, conflict language, or dramatic phrasing, but do not reflect sustained distress (e.g., resolved situations, venting without impairment). This suggests the model sometimes equates emotional intensity with stress. In deployment, this may increase reviewer workload and raise stigma concerns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb9358",
   "metadata": {},
   "source": [
    "### 6.7 Clinical Ethics and Deployment Risks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e743c564",
   "metadata": {},
   "source": [
    "#### 6.7.1 Nature of Model and Risks in Deployment\n",
    "\n",
    "This model detects linguistic patterns statistically associated with stress but it does not diagnose mental health conditions.\n",
    "\n",
    "The combined analyses reveal:\n",
    "\n",
    "* Stress prediction is driven primarily by self focus and negative emotion.\n",
    "\n",
    "* The model performs better for emotionally explicit stress than for subtle or indirect distress.\n",
    "\n",
    "* Domain variation affects performance.\n",
    "\n",
    "* Threshold selection materially changes recall–precision balance.\n",
    "\n",
    "Key risks include:\n",
    "\n",
    "* False Negatives: Distressed individuals may go undetected.\n",
    "\n",
    "* False Positives: Users may be unnecessarily flagged.\n",
    "\n",
    "* Domain Dependence: Contextual differences affect reliability.\n",
    "\n",
    "* Cultural Variation: Stress expression may differ across populations.\n",
    "\n",
    "* Privacy Concerns: Large scale linguistic monitoring raises ethical issues.\n",
    "\n",
    "The model is suitable as a triage or prioritisation tool but should operate within a human-in-the-loop system rather than autonomously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded3b86",
   "metadata": {},
   "source": [
    "#### 6.7.2 Ethical Implications of Observed Errors\n",
    "\n",
    "Confident False Negatives highlight underdetection risk. Subtle or socially embedded distress may not trigger strong negative emotion signals, where individuals express vulnerability within relational contexts. Such errors are clinically significant, as they represent missed opportunities for intervention.\n",
    "\n",
    "\n",
    "Confident False Positives reveal that emotional intensity can be misinterpreted as stress, creating risk of overflagging.\n",
    "\n",
    "\n",
    "These findings demonstrate that model errors reflect structural biases in linguistic weighting rather than random noise. Responsible deployment therefore requires:\n",
    "\n",
    "* Human oversight,\n",
    "* Context-aware review,\n",
    "* Transparent threshold calibration,\n",
    "* Ongoing monitoring for subgroup disparities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06bf266",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6110e00",
   "metadata": {},
   "source": [
    "The final model achieves strong predictive performance (F1 ≈ 0.78), identifying a consistent linguistic signature of stress characterised by heightened self focus and negative emotionality. Its stability on test data suggests reliable internal generalisation within the Dreaddit dataset. However, systematic error patterns reveal structural limitations. Indirect or socially embedded distress is more likely to be missed, while emotionally intense but non-clinical posts are sometimes overflagged. Subreddit variation and threshold sensitivity further demonstrate that stress expression is context dependent rather than uniform.\n",
    "\n",
    "In the Singapore context, where mental health stigma and cultural norms may encourage restrained or indirect expression of distress, these limitations are particularly salient. Automated detection systems risk underidentifying individuals who communicate distress subtly, while overflagging may create unintended consequences in community or campus environments. Moreover, large scale monitoring raises governance concerns around privacy, consent, and proportionality.\n",
    "\n",
    "Instead of being deployed as an autonomous diagnostic system, this model should be positioned as a decision support or triage mechanism within structured institutional settings, such as university wellbeing services, moderated online communities, or early intervention programmes. \n",
    "\n",
    "Policy deployment would require:\n",
    "\n",
    "* Threshold calibration prioritising recall in high risk contexts,\n",
    "\n",
    "* Local validation using Singapore specific linguistic data,\n",
    "\n",
    "* Mandatory human oversight in escalation decisions,\n",
    "\n",
    "* Transparent governance frameworks to mitigate surveillance risk.\n",
    "\n",
    "Ultimately, language based stress detection can augment early identification efforts, but it cannot replace contextual, clinical, or human judgement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
